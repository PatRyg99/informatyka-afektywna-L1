{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aplikacja analizująca emocję streamer’ów na platformie Twitch\n",
    "## Raport z projektu na kurs \"Informatyka afektywna\"\n",
    "### Michał Ilski, Jan Pawłowski, Patryk Rygiel\n",
    "\n",
    "Repozytorium: https://github.com/PatRyg99/informatyka-afektywna-L1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Iteracje projektu\n",
    "\n",
    "1. Etap 1 (26.10.2022)\n",
    "* Zastosowanie mediapipe do ekstrakcji $468$ landmarków twarzy \n",
    "* Klasyfikacja emocji na podstawie landmarków przy użyciu reprezentacji w formie chmur punktów (modele DGCNN)\n",
    "* Demo aplikacji rozpoznającej emocje z kamery\n",
    "\n",
    "2. Etap 2 (21.12.2022)\n",
    "* Analiza dodatkowych zbiorów danych\n",
    "* Modele do uczenia na siatkach 3D (mesh) - FeaSt, SAGE\n",
    "* Reprezentacje cech siatek 3D - HKS (Heat Kernel Signatures), XYZ\n",
    "* Augmentacje danych - interpolacja emocji granicznych, generacja twarzy on nowych emocjach pomiędzy istniejącymi\n",
    "\n",
    "3. Etap 3 (18.01.2022)\n",
    "* Budowa aplikacji do analizy emocji streamer'ów na platformie Twitch\n",
    "* Analiza zmian wykrywanych emocji w trakcie trwania stream'u\n",
    "* Analiza porównawcza rozkładu emocji dla różnych gier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Zbiory danych\n",
    "\n",
    "W ramach zbiorów danych rozważaliśmy trzy poniższe zbiory:\n",
    "* CK+ (Extended Cohn-Kanade dataset) - $7$ emocji + neutral\n",
    "* AffectNet-HQ - $7$ emocji + neutral\n",
    "* AFEW-VA - valence-arousal (regresja)\n",
    "\n",
    "### 2.1 CK+\n",
    "\n",
    "Zbiór zaproponowany do użycia w ramach listy $1$. Zbiór składa się z $593$ sekwencji video dla $123$ różnych osób. Ze wszystkich sekwencji $327$ jest oetykietowanych jedną z z $7$ emocji: anger, contempt, disgust, fear, happy, sadness, surprise. Jedna sekwencja przedstawia przejście z emocji neutralnej do zadanej emocji. Poniższa wizualizacja pokazuje sekwencje $15$ obrazów przejścia z emocji neutral do happy:\n",
    "\n",
    "|  |  |  |  |\n",
    "|--|--|--|--|\n",
    "| ![](images/representation-raport/emotion-sequence/S010_006_00000001.png) | ![](images/representation-raport/emotion-sequence/S010_006_00000002.png) | ![](images/representation-raport/emotion-sequence/S010_006_00000003.png) | ![](images/representation-raport/emotion-sequence/S010_006_00000004.png) |\n",
    "| ![](images/representation-raport/emotion-sequence/S010_006_00000005.png) | ![](images/representation-raport/emotion-sequence/S010_006_00000006.png) | ![](images/representation-raport/emotion-sequence/S010_006_00000007.png) | ![](images/representation-raport/emotion-sequence/S010_006_00000008.png) |\n",
    "| ![](images/representation-raport/emotion-sequence/S010_006_00000009.png) | ![](images/representation-raport/emotion-sequence/S010_006_00000010.png) | ![](images/representation-raport/emotion-sequence/S010_006_00000011.png) | ![](images/representation-raport/emotion-sequence/S010_006_00000012.png) |\n",
    "| ![](images/representation-raport/emotion-sequence/S010_006_00000013.png) | ![](images/representation-raport/emotion-sequence/S010_006_00000014.png) | ![](images/representation-raport/emotion-sequence/S010_006_00000015.png) | |\n",
    "\n",
    "Zbiór jest dosyć tendencyjny ze względu na setting zdjęć: wszystkie są czarno białe, wycentrowane na twarzy oraz twarze są tej samej wielkości. Emocje są za to dobrze oetykietowane.\n",
    "\n",
    "### 2.2 AffectNet-HQ\n",
    "\n",
    "Zbiór AffectNet-HQ był częściowo olabelowany ręcznie, a częściowo automatycznie. Z tego powodu jakość zbioru jest suboptymalna. Przykładem są pokazane zdjęcia.\n",
    "\n",
    "![](images/afektywna-raport/affect-net.png)\n",
    "\n",
    "Mimo tego, że zdjęć jest zdecydowanie więcej niż w CK+, bo aż $31$ tysięcy, jakość annotacji jest na tak słabym poziomie, że użycie zbioru do poprawnej klasyfikacji emocji jest dużo cięższym zadaniem niż ma to miejsce dla zbioru CK+.\n",
    "\n",
    "### 2.3 AFEW-VA\n",
    "\n",
    "Zbiór AFEW-VA składa się z $330$ filmów zaanotowanych przy użyciu valence i arousal. Zbiór jest na tyle problematyczny, że różnica w emocjach na przestrzeni filmów jest dosyć znikoma, oraz emocje są o dosyć małym nasyceniu.\n",
    "\n",
    "![](images/afektywna-raport/afew.png)\n",
    "\n",
    "\n",
    "### 2.4 Wybrany zbiór i podział danych\n",
    "\n",
    "Ze względu na wyżej wymienione problemy ze zbiorami AFEW-VA oraz AffectNet-HQ, zdecydowaliśmy działać tylko na zbiorze CK+.\n",
    "\n",
    "Jako obrazy przedstawiające emocje wybrane zostało ostatnie $20\\%$ klatek z sekwencji jako, że na nich intensywność emocji jest największa i dostajemy parę różnych przykładów emocji dla osoby. Jako, że dla każdej osoby wybierana jest więcej niż jedna klatka z sekwencji oraz dla jednej osoby istnieje z reguły więcej niż jedna sekwencja (rodzaj emocji), zbiór danych został podzielony na poziomie osób, aby uniknąć przelewu danych treningowych do zbioru testowego. Zbiór został podzielony z uwzględnieniem stratyfikacji emocji (na ile to było możliwe) na zbiór treningowy ($85$ osób - $1148$ zdjęć) oraz testowy ($38$ osób - $484$ zdjęć). Poniższy wykres obrazuje rozkład klas w obu zbiorach:\n",
    "\n",
    "![](images/representation-raport/split-dist.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Metodologia\n",
    "\n",
    "![](images/representation-raport/metodology.png)\n",
    "\n",
    "Nasza metoda oparta jest ekstrakcji ze zdjęć tzw. [`FaceMesh`](https://google.github.io/mediapipe/solutions/face_mesh.html#:~:text=MediaPipe%20Face%20Mesh%20is%20a,for%20a%20dedicated%20depth%20sensor.) przy użyciu pre-trenowanego narzędnia `MediaPipe`. `FaceMesh` to reprezentacja twarzy w formie siatki 3D składającej się z $468$ punktów charakterystycznych. Tak uzyskane siatki są używane jako zbiór do trenowania i ewaluacji modeli grafowych, których używamy w tym projekcie.\n",
    "\n",
    "Takie podejście jest dobrą generalizację, gdy mamy mało danych uczących, które są tendencyjne (np. czarno białe, twarz zawsze na środku zdjęcia - problemy zbioru CK+). Model nie overfittuje się do tekstur na zdjęciu, jedyne na czym działa to kształt twarzy.\n",
    "\n",
    "Poniżej przedstawione są przykładowe siatki dla klas emocji na zbiorze CK+:\n",
    "\n",
    "| Anger | Contempt | Disgust | Fear |\n",
    "|--|--|--|--|\n",
    "| ![Anger](images/representation-raport/mesh-emotions/anger.png) | ![Contempt](images/representation-raport/mesh-emotions/contempt.png) | ![Disgust](images/representation-raport/mesh-emotions/disgust.png) | ![Fear](images/representation-raport/mesh-emotions/fear.png) |\n",
    "| Happy | Sadness | Surprise |\n",
    "| ![Happy](images/representation-raport/mesh-emotions/happy.png) | ![Sadness](images/representation-raport/mesh-emotions/sadness.png) | ![Surprise](images/representation-raport/mesh-emotions/surprise.png) |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modelowanie\n",
    "\n",
    "### 4.1 Architektury grafowe\n",
    "Do uczenia na wygenerowanych siatkach 3D, przetestowaliśmy modele na reprezentacji danych wejściowych w postaci chmur punktów (tylko wierzchołki, brak krawędzi) oraz w postaci grafów (wierzchołki i krawędzie) - różnica pomiędzy tymi reprezentacjami jest pokazana na poniższej figurze.\n",
    "\n",
    "![](images/afektywna-raport/mesh-pointcloud.png)\n",
    "\n",
    "Przetestowaliśmy następujące architektury modeli grafowych:\n",
    "\n",
    "#### a) [DGCNN](https://arxiv.org/abs/1801.07829)\n",
    "Model do przetwarzania chmur punktów oparty na dynamicznym grafie budowanym po odległościach wektorów reprezentacji punktów.\n",
    "\n",
    "![](images/afektywna-raport/dgcnn.png)\n",
    "\n",
    "\n",
    "#### b) [GraphSage](https://arxiv.org/abs/1706.02216)\n",
    "Model do przetwarzania grafów bez ukierunkowania na stricte siatki 3D. Agregacje sąsiedztwa jest samplowana z otoczenia - nie muszą być to najbliżsi sąsiedzi.\n",
    "\n",
    "![](images/afektywna-raport/sage.png)\n",
    "\n",
    "\n",
    "#### c) [FeaSt](https://arxiv.org/abs/1706.05206)\n",
    "Model grafowy stworzony w szczególności do przetwarzania siatek 3D w formie grafów. Dla wierzchołka agregowanie są tylko cechy z przylegających ścian.\n",
    "\n",
    "![](images/afektywna-raport/feast.png)\n",
    "\n",
    "### 4.2 Cechy wejściowe\n",
    "\n",
    "Przetestowaliśmy także dwie metody przedstawienia cech wejściowych wierzchołków: \n",
    "* XYZ - koordynaty 3D wierzchołków\n",
    "* HKS - (heat kernel signatures) sygnatury ciepła na siatce, przedstawiają dywergencje gradientu w lokalnym otoczeniu punktu. Sygnatury ciepła uzyskuje się poprzez przejście z bazy cech przestrzennych do bazy cech spektralnych przy użyciu dekompozycji na bazę wektorów własnych operatora Laplace'a-Beltrami'ego. Kolejne wektory własne przedstawiają częstotliwość gradientu. W ramach zadania korzystamy z bazy $16$ wektorów własnych. Poniżej przedstawione są wartości konkretnych wektorów własnych w reprezentacji HKS.\n",
    "\n",
    "| Wektor własny 1 | Wektor własny 5 | Wektor własny 10 | Wektor własny 15 |\n",
    "|--|--|--|--|\n",
    "| ![](images/representation-raport/hks/hks-1.png) | ![](images/representation-raport/hks/hks-5.png) | ![](images/representation-raport/hks/hks-10.png) | ![](images/representation-raport/hks/hks-15.png) |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Augmentacja danych\n",
    "\n",
    "Aby powiększyć zbiór danych oraz dodać bardziej skrajne przypadki w obręnie granic decyzyjnych modelu, zaproponowana została metoda interpolacji pomiędzy emocjami. Nowe próbki są generowane poprzez interpolacje pomiędzy dwoma siatkami - lekkie przesunięcie siatki przedstawiającej jedną emocję w kierunku innej z zachowaniem klasy emocji pierwotnej. W poniższej tabeli pokazane jest przejście z emocji `contempt` do `happy`.\n",
    "\n",
    "|  |  |  |  |  |\n",
    "|--|--|--|--|--|\n",
    "| ![](images/afektywna-raport/interpolate/GIF_Frame%2020.png) | ![](images/afektywna-raport/interpolate/GIF_Frame%2024.png) | ![](images/afektywna-raport/interpolate/GIF_Frame%2027.png) | ![](images/afektywna-raport/interpolate/GIF_Frame%2031.png) | ![](images/afektywna-raport/interpolate/GIF_Frame%2037.png) |\n",
    "| ![](images/afektywna-raport/interpolate/GIF_Frame%2042.png) | ![](images/afektywna-raport/interpolate/GIF_Frame%2045.png) | ![](images/afektywna-raport/interpolate/GIF_Frame%2051.png) | ![](images/afektywna-raport/interpolate/GIF_Frame%2057.png) | ![](images/afektywna-raport/interpolate/GIF_Frame%2062.png) |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Wyniki i ewaluacja modeli\n",
    "\n",
    "| Model | Cechy | Accuracy | F1-macro | Precision-macro | Recall-macro |  \n",
    "|--|--|--|--|--|--|\n",
    "| DGCNN | XYZ | **0.82** | **0.78** | **0.82** | **0.78** |  \n",
    "| DGCNN | HKS | 0.73 | 0.62 | 0.73 | 0.61 |  \n",
    "| | | | | |\n",
    "| SAGE  | XYZ | 0.53 | 0.41 | 0.52 | 0.45 |  \n",
    "| SAGE  | HKS | 0.74 | 0.66 | 0.70 | 0.66 |\n",
    "| | | | | |\n",
    "| FEAST | XYZ | 0.66 | 0.58 | 0.59 | 0.76 |  \n",
    "| FEAST | HKS | 0.79 | 0.74 | 0.77 | 0.74 |  \n",
    "\n",
    "Z naszych eksperymentów wynika, że najlepsze rezultaty osiąga model na reprezentacji w postaci chmur punktów z cechami wejściowymi XYZ. Natomiast dla modeli na siatkach ewidentnie widzimy, że cechy HKS poprawiają zdecydowanie wyniki.\n",
    "\n",
    "![](images/afektywna-raport/confusion-matrix.png)\n",
    "1) anger, 2) contempt, 3) disgust, 4) fear, 5) happy, 6) sadness, 7) surprise.\n",
    "\n",
    "\n",
    "Skupiając się na analizie macierzy pomyłek dla najlepszych modeli tj. `DGCNN-XYZ` oraz `FEAST-HKS`, widzimy, że emocje `happy` oraz `surprise` są bardzo dobrze wykrywane. Emocje negatywne natomiast, są często mylone między sobą - nie są one aż tak charakterystyczne i nie cechują się tak wysoką ekspresją."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Aplikacja analizująca emocje streamer'ów na platformie Twitch - Proof of Concept"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Wczytywanie logów z monitorowanych streamów\n",
    "\n",
    "Zebraliśmy dane monitorując streamerów na platformie Twitch.tv za pomocą napisanej przez nas aplikacji znajdującej się w tym repozytorium.\n",
    "Aplikacja wyświetla stream oraz predykuje emocje z twarzy osoby znajdującej się w zaznaczonym prostokącie. Wyniki zostają zapisane do określinego pliku `.csv` w celu dalszego przetwarzania.\n",
    "\n",
    "![](images/image2.png)\n",
    "![](images/image4.png)\n",
    "![](images/image5.png)\n",
    "![](images/image6.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Analiza emocji w czasie\n",
    "\n",
    "Można zaobserwować, że różne osoby grające w te same gry mają mocno różniące się od siebie przebiegi emocji spredykowane z twarzy.\n",
    "Jest to prawdopodobnie częściowo zależne od faktu, że kształt twarzy ma duży wpływ na działanie modelu, który był uczony na niewielkim zbiorze danych.\n",
    "Co więcej model nie nauczył się poprawnie wykrywać emocji `contempt`.\n",
    "\n",
    "Pomimo tych dużych niedociągnięć na przebiegach można zaobserwować zmiany emocji.\n",
    "Przykładowy przebieg emocji dla krótkiego klipu Youtubera PewDiePie grającego w Subnautica. Pod koniec klipu youtuber zostaje wystraszony, a następnie siedzi niezadowolony.\n",
    "Na modelu widać opisany skok emocji, jednak model niepoprawnie interpretuje przestraszony wyraz twarzy jako połączenie szczęścia i zaskoczenia (i częściowo rozumiemy jego decyzję).\n",
    "Pod sam koniec klipu model całkiem słusznie wskazuje mieszankę złości i smutku.\n",
    "\n",
    "![pewdiepie scream](images/pewdiepie_demo_readings_with_images.png)\n",
    "\n",
    "Wykresy dla analizowanych streamerów znajdują się poniżej:\n",
    "\n",
    "![](images/afektywna-raport/strem-emotions.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Analiza rozkładu emocji\n",
    "\n",
    "W tej części przedstawiony jest rozkład emocji dla streamerów.\n",
    "Można jeszcze lepiej zaobserwować to co było widać w poprzedniej części, a mianowicie, bardzo dużą różnicę w rozkładzie emocji między ludźmi.\n",
    "U niektórych model wykrywa głównie `anger`, u innych głównie `happy`.\n",
    "\n",
    "![](images/afektywna-raport/stream-emotion-dist.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Zestawienie wyników między grami\n",
    "Analizowani streamerzy grali w grę `Genshin Impact` lub `Dota2`. Rozkład emocji jednej oraz drugiej grupy znajduje się na poniższym wykresie.\n",
    "Należy zaznaczyć, że przy tak dużej wariancji emocji między graczami jednej gry należałoby zebrać ogromne ilości danych i włożyć znaczne ilości pracy w dalszą automatyzację tego procesu co wychodzi poza zakres tego projektu.\n",
    "\n",
    "![](images/afektywna-raport/stream-emotion-comparison.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Podsumowanie\n",
    "\n",
    "W ramach projektu zaproponowaliśmy zastosowanie reprezentacji geometrycznej twarzy w postaci chmur punktów i siatek 3D, do klasyfikacji emocji. Przetestowaliśmy różne architektury i podejścia do reprezentacji danych wejściowych. Jako finalny produkt, zbudowaliśmy PoC aplikacji do zbierania i analizy emocji streamer'ów z platformy Twitch."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.16 (main, Jan 11 2023, 16:05:54) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3800b04b570d7adb484cd088406f6e6e787c9e1cbd87758f7af31a0046186547"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
